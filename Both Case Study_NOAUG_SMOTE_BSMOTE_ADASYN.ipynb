{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27680,"status":"ok","timestamp":1694476497349,"user":{"displayName":"JIHOON CHUNG","userId":"01891598578673100541"},"user_tz":240},"id":"0TsnYXNRorSB","outputId":"412350c5-29d8-42a2-885e-26a497f09667"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lzTAiNMW7a9V"},"outputs":[],"source":["import numpy as np\n","from sklearn.manifold import TSNE\n","import os\n","import random\n","import cv2\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","    Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","    Concatenate, multiply, Flatten, BatchNormalization\n","from tensorflow.keras.initializers import glorot_normal\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import save_model\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.applications import ResNet50, imagenet_utils\n","from tensorflow.keras.layers import Softmax\n","from sklearn.metrics import confusion_matrix\n","from imblearn.over_sampling import SMOTE\n","from imblearn.over_sampling import BorderlineSMOTE\n","from imblearn.over_sampling import ADASYN\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.models import load_model\n","import csv\n","from numpy import cov\n","from numpy import trace\n","from numpy import iscomplexobj\n","from numpy import asarray\n","from numpy.random import shuffle\n","from scipy.linalg import sqrtm\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.models import load_model\n","from skimage.transform import resize\n","import natsort\n","import math\n","import glob\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LDuvomeX4d5o"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","\n","  # # It is suggested to use [-1, 1] input for GAN training\n","  # x_train = (x_train.astype('float32') - 127.5) / 127.5\n","  # x_test = (x_test.astype('float32') - 127.5) / 127.5\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  # Get image size\n","  img_size = x_train[0].shape\n","  # Get number of classes\n","  n_classes = len(np.unique(y_train))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/TRAIN/NOAUG')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/FSCORE/NOAUG')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/CLASSIFICATION/NOAUG')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/RECALL/NOAUG')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/PRECISION/NOAUG')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NPGSrms-otKO"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  ada = SMOTE(random_state = 101,k_neighbors=1)\n","\n","\n","  train_rows=len(x_train)\n","  x_train = x_train.reshape(train_rows,-1)\n","  print(x_train.shape)\n","  x_train, y_train =  ada.fit_resample(x_train, y_train)\n","  # print(x_train.shape)\n","  # print(x_train[115,:])\n","  # print(x_train[118,:])\n","  # print(y_train)\n","  x_train = x_train.reshape(-1,64,64,1)\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(c_total/128)\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/TRAIN/BSMOTE')\n","  np.savetxt(\"train_%d_EBM_7.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/FSCORE/BSMOTE')\n","  np.savetxt(\"test_%d_EBM_7.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/CLASSIFICATION/BSMOTE')\n","  np.savetxt(\"classifier_%d_EBM_7.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/RECALL/BSMOTE')\n","  np.savetxt(\"recall_%d_EBM_7.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/PRECISION/BSMOTE')\n","  np.savetxt(\"precision_%d_EBM_7.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0SKb2sixUN7J"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  ada = BorderlineSMOTE(random_state = 101,k_neighbors=1)\n","\n","\n","  train_rows=len(x_train)\n","  x_train = x_train.reshape(train_rows,-1)\n","  print(x_train.shape)\n","  x_train, y_train =  ada.fit_resample(x_train, y_train)\n","  # print(x_train.shape)\n","  # print(x_train[115,:])\n","  # print(x_train[118,:])\n","  # print(y_train)\n","  x_train = x_train.reshape(-1,64,64,1)\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(c_total/128)\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/TRAIN/SMOTE')\n","  np.savetxt(\"train_%d_EBM_7.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/FSCORE/SMOTE')\n","  np.savetxt(\"test_%d_EBM_7.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/CLASSIFICATION/SMOTE')\n","  np.savetxt(\"classifier_%d_EBM_7.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/RECALL/SMOTE')\n","  np.savetxt(\"recall_%d_EBM_7.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/PRECISION/SMOTE')\n","  np.savetxt(\"precision_%d_EBM_7.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ofNLYJP-Art3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vn9I6vRpAryn"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EXzq9ojiAr7Y"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WijGieGXAr99"},"outputs":[],"source":["for number in range(10):\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","\n","  # # It is suggested to use [-1, 1] input for GAN training\n","  # x_train = (x_train.astype('float32') - 127.5) / 127.5\n","  # x_test = (x_test.astype('float32') - 127.5) / 127.5\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  # Get image size\n","  img_size = x_train[0].shape\n","  # Get number of classes\n","  n_classes = len(np.unique(y_train))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/TRAIN/NOAUG')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/FSCORE/NOAUG')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/CLASSIFICATION/NOAUG')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/RECALL/NOAUG')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/PRECISION/NOAUG')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wEd1xlAUAuAI"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  ada = SMOTE(random_state = 101,k_neighbors=7)\n","\n","\n","  train_rows=len(x_train)\n","  x_train = x_train.reshape(train_rows,-1)\n","  print(x_train.shape)\n","  x_train, y_train =  ada.fit_resample(x_train, y_train)\n","  # print(x_train.shape)\n","  # print(x_train[115,:])\n","  # print(x_train[118,:])\n","  # print(y_train)\n","  x_train = x_train.reshape(-1,64,64,1)\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(c_total/128)\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/TRAIN/BSMOTE')\n","  np.savetxt(\"train_%d_EBM_7.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/FSCORE/BSMOTE')\n","  np.savetxt(\"test_%d_EBM_7.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/CLASSIFICATION/BSMOTE')\n","  np.savetxt(\"classifier_%d_EBM_7.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/RECALL/BSMOTE')\n","  np.savetxt(\"recall_%d_EBM_7.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/PRECISION/BSMOTE')\n","  np.savetxt(\"precision_%d_EBM_7.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ywp8XOcYAu79"},"outputs":[],"source":["for number in range(10):\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  ada = BorderlineSMOTE(random_state = 101,k_neighbors=7)\n","\n","\n","  train_rows=len(x_train)\n","  x_train = x_train.reshape(train_rows,-1)\n","  print(x_train.shape)\n","  x_train, y_train =  ada.fit_resample(x_train, y_train)\n","  # print(x_train.shape)\n","  # print(x_train[115,:])\n","  # print(x_train[118,:])\n","  # print(y_train)\n","  x_train = x_train.reshape(-1,64,64,1)\n","\n","  iter_number = int(np.ceil((x_train.shape[0])/batch_number))\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function=[]\n","  precision_loss = []\n","  recall_loss = []\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(c_total/128)\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/TRAIN/SMOTE')\n","  np.savetxt(\"train_%d_EBM_7.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/FSCORE/SMOTE')\n","  np.savetxt(\"test_%d_EBM_7.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/CLASSIFICATION/SMOTE')\n","  np.savetxt(\"classifier_%d_EBM_7.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/RECALL/SMOTE')\n","  np.savetxt(\"recall_%d_EBM_7.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/PRECISION/SMOTE')\n","  np.savetxt(\"precision_%d_EBM_7.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP0g3VPg72WZ8fBi0/jC/tY"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}