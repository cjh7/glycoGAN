{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":711,"status":"ok","timestamp":1690599711421,"user":{"displayName":"JIHOON CHUNG","userId":"01891598578673100541"},"user_tz":240},"id":"1PaQddQgODpn","outputId":"d046d212-ba58-4171-a0c1-6dcd14f13e49"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S6CZ-2bC4-TF"},"outputs":[],"source":["import numpy as np\n","from sklearn.manifold import TSNE\n","import os\n","import random\n","import cv2\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","    Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","    Concatenate, multiply, Flatten, BatchNormalization\n","from tensorflow.keras.initializers import glorot_normal\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import save_model\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.applications import ResNet50, imagenet_utils\n","from tensorflow.keras.layers import Softmax\n","from sklearn.metrics import confusion_matrix\n","\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.models import load_model\n","import csv\n","from numpy import cov\n","from numpy import trace\n","from numpy import iscomplexobj\n","from numpy import asarray\n","from numpy.random import shuffle\n","from scipy.linalg import sqrtm\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.models import load_model\n","from skimage.transform import resize\n","import natsort\n","import math\n","import glob\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5mXLNeZbA_4u"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"fOJN3kuHPGNE"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  batch_size = batch_number\n","\n","  generate_class_a = 151\n","  generate_class_b = 151\n","\n","\n","  iter_number_gan = int(np.ceil((a+b)/batch_number))\n","  iter_number_classification = int(np.ceil((generate_class_a+generate_class_b)/batch_number))\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  # prep data\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_size)\n","  train_iter = iter(ds)\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=True,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss + wrong_loss\n","\n","\n","  def generator_loss(fake_logits):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      return fake_loss\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","\n","\n","  latent_dim=128\n","\n","\n","\n","  def plt_img(generator):\n","      np.random.seed(42)\n","      latent_gen = np.random.normal(size=(n_classes, latent_dim))\n","\n","      x_real = x_test * 0.5 + 0.5\n","      n = n_classes\n","\n","      plt.figure(figsize=(2*n, 2*(n+1)))\n","      for i in range(n):\n","          # display original\n","          ax = plt.subplot(n+1, n, i + 1)\n","          if channel == 3:\n","              plt.imshow(x_real[y_test==i][4].reshape(64, 64, channel))\n","          else:\n","              plt.imshow(x_real[y_test == i][4].reshape(64, 64))\n","              plt.gray()\n","          ax.get_xaxis().set_visible(False)\n","          ax.get_yaxis().set_visible(False)\n","          for c in range(n):\n","              decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])\n","              decoded_imgs = decoded_imgs * 0.5 + 0.5\n","              # display generation\n","              ax = plt.subplot(n+1, n, (i+1)*n + 1 + c)\n","              if channel == 3:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64, channel))\n","              else:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64))\n","                  plt.gray()\n","              ax.get_xaxis().set_visible(False)\n","              ax.get_yaxis().set_visible(False)\n","      #plt.savefig('bagan_gp_results/generated_plot_%d.png' % epoch)\n","      plt.show()\n","      return\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","\n","\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","\n","      for batch in range(iter_number_gan):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  #alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  diff = fake_images - real_images\n","                  interpolated = real_images + alpha * diff\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              g_loss = generator_loss(gen_img_logits)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","  plt_img(generator)\n","\n","  l0_generate=y_train[y_train==0].shape[0]\n","  l1_generate=y_train[y_train==1].shape[0]\n","\n","\n","  generate_0 = generate_class_a-l0_generate\n","  generate_1 = generate_class_b-l1_generate\n","\n","\n","\n","  latent_vectors_0 = tf.random.normal(shape=(generate_0 , latent_dim))\n","  latent_vectors_1 = tf.random.normal(shape=(generate_1 , latent_dim))\n","\n","  label_0 = tf.random.uniform((generate_0,), 0, 1,dtype='int32')\n","  label_1 = tf.random.uniform((generate_1,), 1, 2,dtype='int32')\n","\n","\n","  generate_0_sample = generator.predict([latent_vectors_0, label_0])\n","  generate_1_sample = generator.predict([latent_vectors_1, label_1])\n","\n","\n","  total_train_x = np.concatenate((x_train,generate_0_sample,generate_1_sample),axis=0)\n","  print(total_train_x.shape)\n","  total_train_y = np.concatenate((y_train,label_0,label_1),axis=0)\n","  print(total_train_y.shape)\n","\n","  p = np.random.permutation(len(total_train_y))\n","  total_train_x1 = total_train_x[p,:,:,:]\n","  total_train_y1 = total_train_y[p]\n","\n","  train_x = total_train_x1\n","  train_y = total_train_y1\n","  y_train = train_y\n","\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","  x_train = train_x.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number_classification):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      # classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/TRAIN/BAGANGP')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/FSCORE/BAGANGP')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/CLASSIFICATION/BAGANGP')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/RECALL/BAGANGP')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/PRECISION/BAGANGP')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"II-ngESn6GrT"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  batch_size = batch_number\n","\n","  generate_class_a = 151\n","  generate_class_b = 151\n","\n","\n","  iter_number_gan = int(np.ceil((a+b)/batch_number))\n","  iter_number_classification = int(np.ceil((generate_class_a+generate_class_b)/batch_number))\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  # prep data\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_size)\n","  train_iter = iter(ds)\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss\n","\n","\n","  def generator_loss(fake_logits):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      return fake_loss\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","\n","\n","  latent_dim=128\n","\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","\n","\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","\n","      for batch in range(iter_number_gan):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  #alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha1 = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  #diff = fake_images - real_images\n","                  #interpolated = real_images + alpha * diff\n","                  interpolated = alpha*real_images + (1-alpha) * (real_images+ alpha1)\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              g_loss = generator_loss(gen_img_logits)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","  # plt_img(generator)\n","\n","  l0_generate=y_train[y_train==0].shape[0]\n","  l1_generate=y_train[y_train==1].shape[0]\n","\n","\n","  generate_0 = generate_class_a-l0_generate\n","  generate_1 = generate_class_b-l1_generate\n","\n","\n","\n","  latent_vectors_0 = tf.random.normal(shape=(generate_0 , latent_dim))\n","  latent_vectors_1 = tf.random.normal(shape=(generate_1 , latent_dim))\n","\n","  label_0 = tf.random.uniform((generate_0,), 0, 1,dtype='int32')\n","  label_1 = tf.random.uniform((generate_1,), 1, 2,dtype='int32')\n","\n","\n","  generate_0_sample = generator.predict([latent_vectors_0, label_0])\n","  generate_1_sample = generator.predict([latent_vectors_1, label_1])\n","\n","\n","  total_train_x = np.concatenate((x_train,generate_0_sample,generate_1_sample),axis=0)\n","  print(total_train_x.shape)\n","  total_train_y = np.concatenate((y_train,label_0,label_1),axis=0)\n","  print(total_train_y.shape)\n","\n","  p = np.random.permutation(len(total_train_y))\n","  total_train_x1 = total_train_x[p,:,:,:]\n","  total_train_y1 = total_train_y[p]\n","\n","  train_x = total_train_x1\n","  train_y = total_train_y1\n","  y_train = train_y\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","  x_train = train_x.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number_classification):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      # classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/TRAIN/CDRAGAN')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/FSCORE/CDRAGAN')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/CLASSIFICATION/CDRAGAN')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/RECALL/CDRAGAN')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT/PRECISION/CDRAGAN')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5ObOd45eA6fO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0BuGdE_eA6iM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"2P5y95VtA6kK"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"d4RhSep8A6mw"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  batch_size = batch_number\n","\n","  generate_class_a = 121\n","  generate_class_b = 121\n","\n","\n","  iter_number_gan = int(np.ceil((a+b)/batch_number))\n","  iter_number_classification = int(np.ceil((generate_class_a+generate_class_b)/batch_number))\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  # prep data\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_size)\n","  train_iter = iter(ds)\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=True,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss + wrong_loss\n","\n","\n","  def generator_loss(fake_logits):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      return fake_loss\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","\n","\n","  latent_dim=128\n","\n","\n","\n","  def plt_img(generator):\n","      np.random.seed(42)\n","      latent_gen = np.random.normal(size=(n_classes, latent_dim))\n","\n","      x_real = x_test * 0.5 + 0.5\n","      n = n_classes\n","\n","      plt.figure(figsize=(2*n, 2*(n+1)))\n","      for i in range(n):\n","          # display original\n","          ax = plt.subplot(n+1, n, i + 1)\n","          if channel == 3:\n","              plt.imshow(x_real[y_test==i][4].reshape(64, 64, channel))\n","          else:\n","              plt.imshow(x_real[y_test == i][4].reshape(64, 64))\n","              plt.gray()\n","          ax.get_xaxis().set_visible(False)\n","          ax.get_yaxis().set_visible(False)\n","          for c in range(n):\n","              decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])\n","              decoded_imgs = decoded_imgs * 0.5 + 0.5\n","              # display generation\n","              ax = plt.subplot(n+1, n, (i+1)*n + 1 + c)\n","              if channel == 3:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64, channel))\n","              else:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64))\n","                  plt.gray()\n","              ax.get_xaxis().set_visible(False)\n","              ax.get_yaxis().set_visible(False)\n","      #plt.savefig('bagan_gp_results/generated_plot_%d.png' % epoch)\n","      plt.show()\n","      return\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","\n","\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","\n","      for batch in range(iter_number_gan):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  #alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  diff = fake_images - real_images\n","                  interpolated = real_images + alpha * diff\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              g_loss = generator_loss(gen_img_logits)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","  plt_img(generator)\n","\n","  l0_generate=y_train[y_train==0].shape[0]\n","  l1_generate=y_train[y_train==1].shape[0]\n","\n","\n","  generate_0 = generate_class_a-l0_generate\n","  generate_1 = generate_class_b-l1_generate\n","\n","\n","\n","  latent_vectors_0 = tf.random.normal(shape=(generate_0 , latent_dim))\n","  latent_vectors_1 = tf.random.normal(shape=(generate_1 , latent_dim))\n","\n","  label_0 = tf.random.uniform((generate_0,), 0, 1,dtype='int32')\n","  label_1 = tf.random.uniform((generate_1,), 1, 2,dtype='int32')\n","\n","\n","  generate_0_sample = generator.predict([latent_vectors_0, label_0])\n","  generate_1_sample = generator.predict([latent_vectors_1, label_1])\n","\n","\n","  total_train_x = np.concatenate((x_train,generate_0_sample,generate_1_sample),axis=0)\n","  print(total_train_x.shape)\n","  total_train_y = np.concatenate((y_train,label_0,label_1),axis=0)\n","  print(total_train_y.shape)\n","\n","  p = np.random.permutation(len(total_train_y))\n","  total_train_x1 = total_train_x[p,:,:,:]\n","  total_train_y1 = total_train_y[p]\n","\n","  train_x = total_train_x1\n","  train_y = total_train_y1\n","  y_train = train_y\n","\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","  x_train = train_x.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number_classification):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      # classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/TRAIN/BAGANGP')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/FSCORE/BAGANGP')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/CLASSIFICATION/BAGANGP')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/RECALL/BAGANGP')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/PRECISION/BAGANGP')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hSbuWxu9A7me"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  batch_size = batch_number\n","\n","  generate_class_a = 121\n","  generate_class_b = 121\n","\n","\n","  iter_number_gan = int(np.ceil((a+b)/batch_number))\n","  iter_number_classification = int(np.ceil((generate_class_a+generate_class_b)/batch_number))\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","  # prep data\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_size)\n","  train_iter = iter(ds)\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss\n","\n","\n","  def generator_loss(fake_logits):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      return fake_loss\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","\n","\n","  latent_dim=128\n","\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","\n","\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","\n","      for batch in range(iter_number_gan):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  #alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  alpha1 = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  #diff = fake_images - real_images\n","                  #interpolated = real_images + alpha * diff\n","                  interpolated = alpha*real_images + (1-alpha) * (real_images+ alpha1)\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              g_loss = generator_loss(gen_img_logits)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","\n","\n","  print(d_loss)\n","  print(g_loss)\n","\n","  # plt_img(generator)\n","\n","  l0_generate=y_train[y_train==0].shape[0]\n","  l1_generate=y_train[y_train==1].shape[0]\n","\n","\n","  generate_0 = generate_class_a-l0_generate\n","  generate_1 = generate_class_b-l1_generate\n","\n","\n","\n","  latent_vectors_0 = tf.random.normal(shape=(generate_0 , latent_dim))\n","  latent_vectors_1 = tf.random.normal(shape=(generate_1 , latent_dim))\n","\n","  label_0 = tf.random.uniform((generate_0,), 0, 1,dtype='int32')\n","  label_1 = tf.random.uniform((generate_1,), 1, 2,dtype='int32')\n","\n","\n","  generate_0_sample = generator.predict([latent_vectors_0, label_0])\n","  generate_1_sample = generator.predict([latent_vectors_1, label_1])\n","\n","\n","  total_train_x = np.concatenate((x_train,generate_0_sample,generate_1_sample),axis=0)\n","  print(total_train_x.shape)\n","  total_train_y = np.concatenate((y_train,label_0,label_1),axis=0)\n","  print(total_train_y.shape)\n","\n","  p = np.random.permutation(len(total_train_y))\n","  total_train_x1 = total_train_x[p,:,:,:]\n","  total_train_y1 = total_train_y[p]\n","\n","  train_x = total_train_x1\n","  train_y = total_train_y1\n","  y_train = train_y\n","\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def classifier_loss(true_label,prediction_label):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      return prediction_loss_real\n","\n","\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","  classifier = classifier_gan(cgf, cgs)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","  x_train = train_x.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  for epoch in range(150):\n","      print(epoch)\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number_classification):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","\n","          with tf.GradientTape() as tape_classifier:\n","              real_prediction = classifier([real_images], training=True)\n","\n","              c_loss = classifier_loss(labels,real_prediction)\n","              c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      # classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/TRAIN/CDRAGAN')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/FSCORE/CDRAGAN')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/CLASSIFICATION/CDRAGAN')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/RECALL/CDRAGAN')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT/PRECISION/CDRAGAN')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyM2AHN2btSemnoIfC3eHK9z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}