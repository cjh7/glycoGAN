{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30178,"status":"ok","timestamp":1703459107888,"user":{"displayName":"JIHOON CHUNG","userId":"01891598578673100541"},"user_tz":300},"id":"0mlGxXIZQVvc","outputId":"8929028c-630f-481c-b85f-746abac69f18"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFE4uNk5CqkV"},"outputs":[],"source":["import numpy as np\n","from sklearn.manifold import TSNE\n","import os\n","import random\n","import cv2\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model, Sequential\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","    Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","    Concatenate, multiply, Flatten, BatchNormalization\n","from tensorflow.keras.initializers import glorot_normal\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import save_model\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import precision_score\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.applications import ResNet50, imagenet_utils\n","from tensorflow.keras.layers import Softmax\n","from sklearn.metrics import confusion_matrix\n","\n","from sklearn.manifold import TSNE\n","from tensorflow.keras.models import load_model\n","import csv\n","from numpy import cov\n","from numpy import trace\n","from numpy import iscomplexobj\n","from numpy import asarray\n","from numpy.random import shuffle\n","from scipy.linalg import sqrtm\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.models import load_model\n","from skimage.transform import resize\n","import natsort\n","import math\n","import glob\n","import pandas as pd"]},{"cell_type":"code","source":["for number in range(10):\n","\n","\n","  gamma=0.1\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,0]\n","    phase_1[a,:]=result\n","    print(result[799])\n","    a=a+1\n","\n","\n"],"metadata":{"id":"aYjCDrPfbEok"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"m9CdomElQgxT"},"outputs":[],"source":["for number in range(10):\n","\n","\n","  gamma=0.1\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=False,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def discriminator_loss(real_logits, fake_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      return  fake_loss +   real_loss\n","\n","\n","\n","\n","\n","  def generator_loss(fake_logits,fake_labels,fake_prediction,epoch):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      abnormal_label=np.where((fake_labels==1))\n","\n","      fake_labels_array=np.array(fake_labels)\n","      fake_prediction_array=np.array(fake_prediction)\n","      prediction_loss_gen=tf.reduce_mean(scce((fake_labels_array[abnormal_label]),(fake_prediction_array[abnormal_label,:])))\n","\n","      num = 0\n","      summation = 0\n","      for i in range(fake_prediction.shape[0]):\n","        if fake_labels[i]==1:\n","          if (tf.math.argmax(fake_prediction[i,:]))==1:\n","              num = num +1\n","              order = tf.sort(fake_prediction[i,:], direction='DESCENDING').numpy()\n","              sum =  (order[0]-order[1])*(order[0]-order[1])\n","              summation = summation + sum\n","\n","\n","      if num ==0:\n","        boundary = 0\n","      else:\n","        boundary = summation /num\n","      lambda1 = np.power(gamma,epoch)\n","\n","      return fake_loss,prediction_loss_gen,boundary, fake_loss + (1-lambda1)*prediction_loss_gen + (lambda1)*boundary\n","\n","\n","\n","  # Define the loss functions to be used for generator\n","  def classifier_loss(true_label,prediction_label,classify_0, fake_prediction_0,classify_1, fake_prediction_1):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      prediction_loss_fake_0=tf.reduce_mean(scce(classify_0,fake_prediction_0))\n","      prediction_loss_fake_1=tf.reduce_mean(scce(classify_1,fake_prediction_1))\n","\n","\n","\n","      num=0\n","      summation = 0\n","      for i in range(fake_prediction_1.shape[0]):\n","        if (tf.math.argmax(fake_prediction_1[i,:]))==1:\n","            num = num +1\n","            order = tf.sort(fake_prediction_1[i,:], direction='DESCENDING').numpy()\n","            sum =  (order[0]-order[1])*(order[0]-order[1])\n","            summation = summation + sum\n","\n","      if num ==0:\n","        boundary = 0\n","      else:\n","        boundary = summation /num\n","      lambda1 = np.power(gamma,epoch)\n","\n","\n","\n","\n","      return prediction_loss_real+ (1-lambda1)*(prediction_loss_fake_1)  + (lambda1)*boundary\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","\n","  classifier = classifier_gan(cgf, cgs)\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  batch_size=batch_number\n","  latent_dim=128\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  for epoch in range(150):\n","      print(\"epoch\")\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","                  d_cost = discriminator_loss(real_logits,fake_logits)\n","                  d_loss = d_cost\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              gen_prediction = classifier([generated_images], training=True)\n","\n","              fake_loss,prediction_loss_gen,boundary,g_loss = generator_loss(gen_img_logits,fake_labels,gen_prediction,epoch)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","          l0=labels[labels==0].shape[0]\n","          l1=labels[labels==1].shape[0]\n","\n","\n","          # classify_train_l0 = class_number-l0\n","          # classify_train_l1 = class_number-l1\n","          # classify_train_l2 = class_number-l2\n","\n","          classify_train_l0 = l0-l0\n","          classify_train_l1 = l0-l1\n","\n","\n","\n","\n","      #     ########################### Train the Classifier ###########################\n","          random_latent_vectors_0 = tf.random.normal(shape=(classify_train_l0, latent_dim))\n","          random_latent_vectors_1 = tf.random.normal(shape=(classify_train_l1, latent_dim))\n","          classify_0 = tf.random.uniform((classify_train_l0,), 0, 1,dtype='int32')\n","          classify_1 = tf.random.uniform((classify_train_l1,), 1, 2,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape_classifier:\n","\n","            fake_prediction_000 = generator([random_latent_vectors_0, classify_0], training=True)\n","            fake_prediction_111 = generator([random_latent_vectors_1, classify_1], training=True)\n","\n","\n","            real_prediction = classifier([real_images], training=True)\n","            fake_prediction_0 = classifier([fake_prediction_000], training=True)\n","            fake_prediction_1 = classifier([fake_prediction_111], training=True)\n","\n","\n","\n","            c_loss = classifier_loss(labels,real_prediction,classify_0, fake_prediction_0,classify_1, fake_prediction_1)\n","            c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(d_loss)\n","      # print(g_loss)\n","      # print(c_loss)\n","\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/TRAIN/COOPERATIVE')\n","  np.savetxt(\"train_%d_EBM_1.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/FSCORE/COOPERATIVE')\n","  np.savetxt(\"test_%d_EBM_1.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/CLASSIFICATION/COOPERATIVE')\n","  np.savetxt(\"classifier_%d_EBM_1.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/RECALL/COOPERATIVE')\n","  np.savetxt(\"recall_%d_EBM_1.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/PRECISION/COOPERATIVE')\n","  np.savetxt(\"precision_%d_EBM_1.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nGCFLsQvr1t"},"outputs":[],"source":["for number in range(1):\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/phase_EXP3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=None)\n","    result = result.values[:,1]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  #phase 1:  zero: 58 & one: 38\n","  #phase 2:  zero: 64 & one: 32\n","  #phase 3:  zero: 59 & one: 37\n","\n","  #total: zero: 181 & one: 107\n","  phase_zero=np.zeros((181,800))\n","  phase_one=np.zeros((107,800))\n","\n","\n","  j=0\n","  for i in list(range(57))+list(range(58,59)):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=58\n","  for i in list(range(60))+list(range(61,64))+list(range(65,66)):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=122\n","  for i in list(range(57))+list(range(58,60)):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  j=0\n","  for i in list(range(57,58))+list(range(59,96)):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=38\n","  for i in list(range(60,61))+list(range(64,65))+list(range(66,96)):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=70\n","  for i in list(range(57,58))+list(range(60,96)):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_zero)\n","  maximum=np.max(phase_zero)\n","\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((181,28,28,1))\n","  line_crop_up=np.zeros((107,28,28,1))\n","\n","\n","  for i in range(181):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(107):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 150\n","  b = 4\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=False,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss +wrong_loss\n","\n","\n","\n","\n","\n","  def generator_loss(fake_logits,fake_labels,fake_prediction,epoch):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      prediction_loss_gen=tf.reduce_mean(scce(fake_labels,fake_prediction))\n","      boundary = 0\n","      return fake_loss,prediction_loss_gen,boundary,fake_loss + prediction_loss_gen +  boundary\n","\n","\n","\n","  # Define the loss functions to be used for generator\n","  def classifier_loss(true_label,prediction_label,classify_0, fake_prediction_0,classify_1, fake_prediction_1):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      prediction_loss_fake_0=tf.reduce_mean(scce(classify_0,fake_prediction_0))\n","      prediction_loss_fake_1=tf.reduce_mean(scce(classify_1,fake_prediction_1))\n","\n","      return prediction_loss_real + prediction_loss_fake_0+prediction_loss_fake_1\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","\n","  classifier = classifier_gan(cgf, cgs)\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  batch_size=batch_number\n","  latent_dim=128\n","\n","\n","\n","  def plt_img(generator):\n","      np.random.seed(42)\n","      latent_gen = np.random.normal(size=(n_classes, latent_dim))\n","\n","      x_real = x_test * 0.5 + 0.5\n","      n = n_classes\n","\n","      plt.figure(figsize=(2*n, 2*(n+1)))\n","      for i in range(n):\n","          # display original\n","          ax = plt.subplot(n+1, n, i + 1)\n","          if channel == 3:\n","              plt.imshow(x_real[y_test==i][1].reshape(64, 64, channel))\n","          else:\n","              plt.imshow(x_real[y_test == i][1].reshape(64, 64))\n","              plt.gray()\n","          ax.get_xaxis().set_visible(False)\n","          ax.get_yaxis().set_visible(False)\n","          for c in range(n):\n","              #decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])\n","              decoded_imgs = generator([latent_gen, np.ones(n)*c],training=True)\n","              decoded_imgs=np.asarray(decoded_imgs)\n","              decoded_imgs = decoded_imgs * 0.5 + 0.5\n","              # display generation\n","              ax = plt.subplot(n+1, n, (i+1)*n + 1 + c)\n","              if channel == 3:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64, channel))\n","              else:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64))\n","                  plt.gray()\n","              ax.get_xaxis().set_visible(False)\n","              ax.get_yaxis().set_visible(False)\n","      # plt.savefig('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/code/eps_tsne/generated_plot_%d.svg' % epoch,format='svg', dpi=1200)\n","      # plt.show()\n","      return\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function = []\n","  recall_loss = []\n","  precision_loss = []\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  for epoch in range(150):\n","      print(\"epoch\")\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  diff = fake_images - real_images\n","                  interpolated = real_images + alpha * diff\n","\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              gen_prediction = classifier([generated_images], training=True)\n","\n","              fake_loss,prediction_loss_gen,boundary,g_loss = generator_loss(gen_img_logits,fake_labels,gen_prediction,epoch)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","          l0=labels[labels==0].shape[0]\n","          l1=labels[labels==1].shape[0]\n","\n","          classify_train_l0 = l0-l0\n","          classify_train_l1 = l0-l1\n","\n","      #     ########################### Train the Classifier ###########################\n","          random_latent_vectors_0 = tf.random.normal(shape=(classify_train_l0, latent_dim))\n","          random_latent_vectors_1 = tf.random.normal(shape=(classify_train_l1, latent_dim))\n","          classify_0 = tf.random.uniform((classify_train_l0,), 0, 1,dtype='int32')\n","          classify_1 = tf.random.uniform((classify_train_l1,), 1, 2,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape_classifier:\n","\n","            fake_prediction_000 = generator([random_latent_vectors_0, classify_0], training=True)\n","            fake_prediction_111 = generator([random_latent_vectors_1, classify_1], training=True)\n","\n","\n","            real_prediction = classifier([real_images], training=True)\n","            fake_prediction_0 = classifier([fake_prediction_000], training=True)\n","            fake_prediction_1 = classifier([fake_prediction_111], training=True)\n","\n","            c_loss = classifier_loss(labels,real_prediction,classify_0, fake_prediction_0,classify_1, fake_prediction_1)\n","            c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(d_loss)\n","      # print(g_loss)\n","      # print(c_loss)\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","      if epoch%5 ==0:\n","        plt_img(generator)\n","        plt.figure(figsize=(8, 8))\n","        color = plt.get_cmap('tab10')\n","        A=tf.concat([fake_prediction_000,fake_prediction_111,real_images],0)\n","        B=tf.concat([classify_0,classify_1,labels],0)\n","        latent = cgf.predict(A)  # with Encoder\n","        tsne_model = TSNE(n_components=2, init='random', random_state=0)\n","        new_values = tsne_model.fit_transform(latent)\n","        x = []\n","        y = []\n","        for value in new_values:\n","          x.append(value[0])\n","          y.append(value[1])\n","        x = np.array(x)\n","        y = np.array(y)\n","\n","        x_generated = x[:-1 * batch_size]\n","        y_generated = y[:-1 * batch_size]\n","        x_real = x[-1 * batch_size:]\n","        y_real = y[-1 * batch_size:]\n","        generated_label = B[:-1 * batch_size]\n","        real_label = B[-1 * batch_size:]\n","\n","\n","        loop = 0\n","        markers = ['o', 'x']\n","        for x, y, l in [(x_real, y_real, real_label), (x_generated, y_generated, generated_label)]:\n","          marker = markers[loop]\n","          loop += 1\n","          print(l)\n","          # print(batch_size)\n","          # print(x_real.shape)\n","          # print(real_label.shape)\n","          for c in range(n_classes):\n","              l3=np.asarray(l)\n","              plt.scatter(x[l3 == c], y[l3 == c], marker=marker, s=50, c=np.array([color(c)]))\n","        plt.legend([ \"Actual_Solution\",\"Actual_Gel\", \"Generated_Solution\",\"Generated_Gel\"],fontsize=16)\n","        image_name = 'new_pf_%d.eps' %epoch\n","        os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/code/eps_tsne')\n","        plt.savefig(image_name, format='eps', dpi=1200)\n","        plt.show()\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/TRAIN/PROPOSE')\n","  # np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/FSCORE/PROPOSE')\n","  # np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/CLASSIFICATION/PROPOSE')\n","  # np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/RECALL/PROPOSE')\n","  # np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/imbalanceGAN_scale/imbalanceGAN/RESULT_0911/RESULT/PRECISION/PROPOSE')\n","  # np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jH1Yh0_yBFj1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3Hrlx2kPBFoX"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OXBni27XBFte"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"T_P8yz16BFwe"},"outputs":[],"source":["for number in range(10):\n","\n","\n","  gamma=0.1\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=False,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def discriminator_loss(real_logits, fake_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      return  fake_loss +   real_loss\n","\n","\n","\n","\n","\n","  def generator_loss(fake_logits,fake_labels,fake_prediction,epoch):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      abnormal_label=np.where((fake_labels==1))\n","\n","      fake_labels_array=np.array(fake_labels)\n","      fake_prediction_array=np.array(fake_prediction)\n","      prediction_loss_gen=tf.reduce_mean(scce((fake_labels_array[abnormal_label]),(fake_prediction_array[abnormal_label,:])))\n","\n","      num = 0\n","      summation = 0\n","      for i in range(fake_prediction.shape[0]):\n","        if fake_labels[i]==1:\n","          if (tf.math.argmax(fake_prediction[i,:]))==1:\n","              num = num +1\n","              order = tf.sort(fake_prediction[i,:], direction='DESCENDING').numpy()\n","              sum =  (order[0]-order[1])*(order[0]-order[1])\n","              summation = summation + sum\n","\n","\n","      if num ==0:\n","        boundary = 0\n","      else:\n","        boundary = summation /num\n","      lambda1 = np.power(gamma,epoch)\n","\n","      return fake_loss,prediction_loss_gen,boundary, fake_loss + (1-lambda1)*prediction_loss_gen + (lambda1)*boundary\n","\n","\n","\n","  # Define the loss functions to be used for generator\n","  def classifier_loss(true_label,prediction_label,classify_0, fake_prediction_0,classify_1, fake_prediction_1):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      prediction_loss_fake_0=tf.reduce_mean(scce(classify_0,fake_prediction_0))\n","      prediction_loss_fake_1=tf.reduce_mean(scce(classify_1,fake_prediction_1))\n","\n","\n","\n","      num=0\n","      summation = 0\n","      for i in range(fake_prediction_1.shape[0]):\n","        if (tf.math.argmax(fake_prediction_1[i,:]))==1:\n","            num = num +1\n","            order = tf.sort(fake_prediction_1[i,:], direction='DESCENDING').numpy()\n","            sum =  (order[0]-order[1])*(order[0]-order[1])\n","            summation = summation + sum\n","\n","      if num ==0:\n","        boundary = 0\n","      else:\n","        boundary = summation /num\n","      lambda1 = np.power(gamma,epoch)\n","\n","\n","\n","\n","      return prediction_loss_real+ (1-lambda1)*(prediction_loss_fake_1)  + (lambda1)*boundary\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","\n","  classifier = classifier_gan(cgf, cgs)\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  batch_size=batch_number\n","  latent_dim=128\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  precision_loss=[]\n","  recall_loss=[]\n","  classifier_loss_function = []\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  for epoch in range(150):\n","      print(\"epoch\")\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              wrong_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","                  d_cost = discriminator_loss(real_logits,fake_logits)\n","                  d_loss = d_cost\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              gen_prediction = classifier([generated_images], training=True)\n","\n","              fake_loss,prediction_loss_gen,boundary,g_loss = generator_loss(gen_img_logits,fake_labels,gen_prediction,epoch)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","          l0=labels[labels==0].shape[0]\n","          l1=labels[labels==1].shape[0]\n","\n","\n","          # classify_train_l0 = class_number-l0\n","          # classify_train_l1 = class_number-l1\n","          # classify_train_l2 = class_number-l2\n","\n","          classify_train_l0 = l0-l0\n","          classify_train_l1 = l0-l1\n","\n","\n","\n","\n","      #     ########################### Train the Classifier ###########################\n","          random_latent_vectors_0 = tf.random.normal(shape=(classify_train_l0, latent_dim))\n","          random_latent_vectors_1 = tf.random.normal(shape=(classify_train_l1, latent_dim))\n","          classify_0 = tf.random.uniform((classify_train_l0,), 0, 1,dtype='int32')\n","          classify_1 = tf.random.uniform((classify_train_l1,), 1, 2,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape_classifier:\n","\n","            fake_prediction_000 = generator([random_latent_vectors_0, classify_0], training=True)\n","            fake_prediction_111 = generator([random_latent_vectors_1, classify_1], training=True)\n","\n","\n","            real_prediction = classifier([real_images], training=True)\n","            fake_prediction_0 = classifier([fake_prediction_000], training=True)\n","            fake_prediction_1 = classifier([fake_prediction_111], training=True)\n","\n","\n","\n","            c_loss = classifier_loss(labels,real_prediction,classify_0, fake_prediction_0,classify_1, fake_prediction_1)\n","            c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(d_loss)\n","      # print(g_loss)\n","      # print(c_loss)\n","\n","\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/TRAIN/COOPERATIVE')\n","  np.savetxt(\"train_%d_EBM_1.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/FSCORE/COOPERATIVE')\n","  np.savetxt(\"test_%d_EBM_1.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/CLASSIFICATION/COOPERATIVE')\n","  np.savetxt(\"classifier_%d_EBM_1.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/RECALL/COOPERATIVE')\n","  np.savetxt(\"recall_%d_EBM_1.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/PRECISION/COOPERATIVE')\n","  np.savetxt(\"precision_%d_EBM_1.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I0I7WlKdBJON"},"outputs":[],"source":["for number in range(10):\n","\n","\n","\n","\n","  print(\"number\")\n","  print(number)\n","  SEED = number*200\n","\n","  tf.keras.utils.set_random_seed(SEED)\n","  tf.config.experimental.enable_op_determinism()\n","  TF_CUDNN_DETERMINISM=SEED\n","\n","  import numpy as np\n","  np.random.seed(1000*number)\n","  from sklearn.manifold import TSNE\n","  import os\n","  import random\n","  import cv2\n","  import tensorflow as tf\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","  import tensorflow.keras.backend as K\n","  from tensorflow.keras import Model, Sequential\n","  from tensorflow.keras.initializers import RandomNormal\n","  from tensorflow.keras.layers import Input, Reshape, Dense, Dropout, \\\n","      Activation, LeakyReLU, Conv2D, Conv2DTranspose, Embedding, \\\n","      Concatenate, multiply, Flatten, BatchNormalization\n","  from tensorflow.keras.initializers import glorot_normal\n","  from tensorflow.keras.optimizers import Adam\n","  from sklearn.model_selection import train_test_split\n","  from tensorflow.keras.models import save_model\n","  from tensorflow.keras.models import load_model\n","  from sklearn.metrics import f1_score\n","  from sklearn.metrics import recall_score\n","  from sklearn.metrics import precision_score\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.layers import GlobalAveragePooling2D\n","  from tensorflow.keras.applications import ResNet50, imagenet_utils\n","  from tensorflow.keras.layers import Softmax\n","  from sklearn.metrics import confusion_matrix\n","\n","  from sklearn.manifold import TSNE\n","  from tensorflow.keras.models import load_model\n","  import csv\n","  from numpy import cov\n","  from numpy import trace\n","  from numpy import iscomplexobj\n","  from numpy import asarray\n","  from numpy.random import shuffle\n","  from scipy.linalg import sqrtm\n","  from tensorflow.keras.applications.inception_v3 import InceptionV3\n","  from tensorflow.keras.applications.inception_v3 import preprocess_input\n","  from tensorflow.keras.models import load_model\n","  from skimage.transform import resize\n","  import natsort\n","  import math\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def sigmoid(x):\n","      sig = 1 / (1 + np.exp(-x))\n","      return sig\n","\n","\n","  os.environ['PYTHONHASHSEED'] = str(SEED)\n","  random.seed(SEED)\n","  #np.random.seed(SEED)\n","  tf.random.set_seed(SEED)\n","  weight_init = glorot_normal(seed=SEED)\n","\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum1')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_1 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_1[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum2')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_2 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_2[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco/imbalanceGAN/data/glyco_spectrum3')\n","  path = os.getcwd()\n","  filelist = glob.glob(os.path.join(path, \"*.csv\"))\n","  filelist=sorted(filelist)\n","  a = 0\n","  phase_3 = np.zeros((96,800))\n","\n","  for file in filelist:\n","    print(file)\n","    result = pd.read_csv(file,header=0)\n","    result = result.values[:,2]\n","    phase_3[a,:]=result\n","    #print(result.shape)\n","    #print(result)\n","    a=a+1\n","    # a = a +np.mean(result[298:299])\n","    #phase.append(result)\n","\n","  #total: zero: 194 & one: 94\n","  phase_zero=np.zeros((194,800))\n","  phase_one=np.zeros((94,800))\n","\n","  j=0\n","  for i in range(66):\n","    phase_zero[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=66\n","  for i in range(64):\n","    phase_zero[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=130\n","  for i in  range(64):\n","    phase_zero[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","  j=0\n","  for i in range(66,96):\n","    phase_one[j,:]=phase_1[i,:]\n","    j=j+1\n","  j=30\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_2[i,:]\n","    j=j+1\n","  j=62\n","  for i in range(64,96):\n","    phase_one[j,:]=phase_3[i,:]\n","    j=j+1\n","\n","\n","  print(np.max(phase_zero))\n","  print(np.max(phase_one))\n","  print(np.min(phase_zero))\n","  print(np.min(phase_one))\n","\n","  minimum=np.min(phase_one)\n","  maximum=np.max(phase_zero)\n","\n","\n","\n","  phase_zero_scale= (phase_zero-((maximum+minimum)/2))/((maximum-minimum)/2)\n","  phase_one_scale= (phase_one-((maximum+minimum)/2))/((maximum-minimum)/2)\n","\n","\n","  ring_crop_up=np.zeros((194,28,28,1))\n","  line_crop_up=np.zeros((94,28,28,1))\n","\n","\n","  for i in range(194):\n","    ring_crop_up[i,:,:,:]=np.reshape(phase_zero_scale[i,0:784], (28,28,1))\n","  for i in range(94):\n","    line_crop_up[i,:,:,:]=np.reshape(phase_one_scale[i,0:784], (28,28,1))\n","\n","\n","\n","\n","  # os.chdir('/content/drive/My Drive/Colab Notebooks/SUBMISSION_CODE/EBM/DATA')\n","\n","\n","\n","  def change_image_shape(images):\n","      shape_tuple = images.shape\n","      if len(shape_tuple) == 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], 1)\n","      elif shape_tuple == 4 and shape_tuple[-1] > 3:\n","          images = images.reshape(-1, shape_tuple[-1], shape_tuple[-1], shape_tuple[1])\n","      return images\n","\n","\n","\n","  ring_crop = ring_crop_up\n","  line_crop = line_crop_up\n","\n","\n","\n","  print(ring_crop.shape)\n","  print(line_crop.shape)\n","\n","\n","\n","\n","  normal_image = ring_crop\n","  fan_image = line_crop\n","\n","\n","\n","  np.random.seed(number*100)\n","  np.random.shuffle(normal_image)\n","  np.random.shuffle(fan_image)\n","\n","\n","\n","  a = 120\n","  b = 10\n","\n","\n","  batch_number = 30\n","  class_number = 30\n","\n","  iter_number = int(np.ceil((a+b)/batch_number))\n","\n","  normal = normal_image[0:a,:,:,:]\n","  fan = fan_image[0:b,:,:,:]\n","\n","\n","  normal_test = normal_image[a:,:,:,:]\n","  fan_test = fan_image[b:,:,:,:]\n","\n","\n","\n","\n","\n","  images =  np.concatenate((normal,fan),axis=0)\n","  normal_label = np.zeros(normal.shape[0])\n","  fan_label = np.ones(fan.shape[0])\n","  labels =np.concatenate((normal_label,fan_label),axis=0)\n","\n","\n","  images_test =  np.concatenate((normal_test,fan_test),axis=0)\n","  normal_test_label = np.zeros(normal_test.shape[0])\n","  fan_test_label = np.ones(fan_test.shape[0])\n","  labels_test =np.concatenate((normal_test_label,fan_test_label),axis=0)\n","\n","  images = change_image_shape(images)\n","  images_test = change_image_shape(images_test)\n","\n","\n","  channel = images.shape[-1]\n","\n","  real = np.ndarray(shape=(images.shape[0], 64, 64, channel))\n","  for i in range(images.shape[0]):\n","      real[i] = cv2.resize(images[i], (64, 64)).reshape((64, 64, channel))\n","\n","\n","  # to 64 x 64 x channel\n","  real_test = np.ndarray(shape=(images_test.shape[0], 64, 64, channel))\n","  for i in range(images_test.shape[0]):\n","      real_test[i] = cv2.resize(images_test[i], (64, 64)).reshape((64, 64, channel))\n","\n","  x_train, x_test, y_train, y_test = train_test_split(real, labels, test_size=0.01, shuffle=True, random_state=42)\n","  x_test, x_valid, y_test, y_valid = train_test_split(real_test, labels_test, test_size=0.01, shuffle=True, random_state=42)\n","\n","\n","  img_size = x_train[0].shape\n","  n_classes = len(np.unique(y_train))\n","\n","\n","  # %% ---------------------------------- Hyperparameters ----------------------------------------------------------------\n","\n","  optimizer = Adam(lr=0.0002, beta_1=0.5, beta_2=0.9)\n","  latent_dim=128\n","\n","  # trainRatio === times(Train D) / times(Train G)\n","  trainRatio = 5\n","\n","  # %% ---------------------------------- Models Setup -------------------------------------------------------------------\n","  # Build Generator/Decoder\n","  def decoder():\n","      # weight initialization\n","      init = RandomNormal(stddev=0.02,seed=10)\n","\n","      noise_le = Input((latent_dim,))\n","      x = Dense(4*4*256,kernel_initializer=init,bias_initializer=init)(noise_le)\n","      x = LeakyReLU(alpha=0.2)(x)\n","      x = Reshape((4, 4, 256))(x)\n","      x = Conv2DTranspose(filters=128,kernel_size=(4, 4),strides=(2, 2),padding='same',kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = BatchNormalization(beta_initializer='zeros', gamma_initializer='ones',moving_mean_initializer='zeros',moving_variance_initializer='ones',)(x)\n","      x = LeakyReLU(0.2)(x)\n","      generated = Conv2DTranspose(channel, (4, 4), strides=(2, 2), padding='same', activation='tanh', kernel_initializer=init,bias_initializer=init)(x)\n","\n","      generator = Model(inputs=noise_le, outputs=generated)\n","      return generator\n","\n","  # Build Encoder\n","  def encoder():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      feature = Flatten()(x)\n","      feature = Dense(latent_dim,kernel_initializer=init,bias_initializer=init)(feature)\n","      out = LeakyReLU(0.2)(feature)\n","      model = Model(inputs=img, outputs=out)\n","      return model\n","\n","  # Build Embedding model\n","  def embedding_labeled_latent():\n","      label = Input((1,), dtype='int32')\n","      noise = Input((latent_dim,))\n","      le = Flatten()(Embedding(n_classes, latent_dim)(label))\n","      noise_le = multiply([noise, le])\n","      model = Model([noise, label], noise_le)\n","      return model\n","\n","  # Build Autoencoder\n","  def autoencoder_trainer(encoder, decoder, embedding):\n","\n","      label = Input((1,), dtype='int32')\n","      img = Input(img_size)\n","      latent = encoder(img)\n","      labeled_latent = embedding([latent, label])\n","      rec_img = decoder(labeled_latent)\n","      model = Model([img, label], rec_img)\n","      model.compile(optimizer=optimizer, loss='mae')\n","      return model\n","\n","\n","  en = encoder()\n","  deca = decoder()\n","  em = embedding_labeled_latent()\n","  ae = autoencoder_trainer(en, deca, em)\n","\n","\n","  ae.fit([x_train, y_train], x_train,\n","        epochs=100,\n","        batch_size=batch_number,\n","        shuffle=False,\n","        validation_data=([x_test, y_test], x_test),verbose=2)\n","\n","\n","  def discriminator_cwgan():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      label = Input((1,), dtype='int32')\n","\n","      x = Conv2D(64, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Flatten()(x)\n","      le = Flatten()(Embedding(n_classes, 512)(label))\n","      le = Dense(4 * 4 * 256, kernel_initializer=init,bias_initializer=init)(le)\n","      le = LeakyReLU(0.2)(le)\n","      x_y = multiply([x, le])\n","      x_y = Dense(512, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out = Dense(1, kernel_initializer=init,bias_initializer=init)(x_y)\n","      model = Model(inputs=[img, label], outputs=out)\n","\n","      return model\n","\n","  scce = tf.keras.losses.SparseCategoricalCrossentropy()\n","  def classifier_gan_first():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img = Input(img_size)\n","      x = Conv2D(32, kernel_size=(4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(img)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(32, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(128, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      x = Conv2D(256, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init,bias_initializer=init)(x)\n","      x = LeakyReLU(0.2)(x)\n","      out = Flatten()(x)\n","      model = Model(inputs=[img], outputs=out)\n","      return model\n","\n","  def classifier_gan_second():\n","      init = RandomNormal(stddev=0.02,seed=10)\n","      img1 = Input(4096)\n","      x_y = Dense(100, kernel_initializer=init,bias_initializer=init)(img1)\n","      out = Dense(n_classes, kernel_initializer=init,bias_initializer=init)(x_y)\n","      out=Softmax()(out)\n","      model = Model(inputs=[img1], outputs=out)\n","      return model\n","\n","\n","  def classifier_gan(classifier_gan_first,classifier_gan_second):\n","      img = Input(img_size)\n","      latent = classifier_gan_first(img)\n","      out= classifier_gan_second([latent])\n","      model = Model([img], out)\n","\n","      return model\n","\n","  def discriminator_loss(real_logits, fake_logits, wrong_label_logits ):\n","      real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=real_logits, labels=tf.ones_like(real_logits)))\n","      fake_loss= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.zeros_like(fake_logits)))\n","      wrong_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=wrong_label_logits, labels=tf.zeros_like(wrong_label_logits)))\n","\n","      return  fake_loss +   real_loss +wrong_loss\n","\n","\n","\n","\n","\n","  def generator_loss(fake_logits,fake_labels,fake_prediction,epoch):\n","      fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=fake_logits, labels=tf.ones_like(fake_logits)))\n","      prediction_loss_gen=tf.reduce_mean(scce(fake_labels,fake_prediction))\n","      boundary = 0\n","      return fake_loss,prediction_loss_gen,boundary,fake_loss + prediction_loss_gen +  boundary\n","\n","\n","\n","  # Define the loss functions to be used for generator\n","  def classifier_loss(true_label,prediction_label,classify_0, fake_prediction_0,classify_1, fake_prediction_1):\n","      prediction_loss_real=tf.reduce_mean(scce(true_label,prediction_label))\n","      prediction_loss_fake_0=tf.reduce_mean(scce(classify_0,fake_prediction_0))\n","      prediction_loss_fake_1=tf.reduce_mean(scce(classify_1,fake_prediction_1))\n","\n","\n","      return prediction_loss_real + prediction_loss_fake_0+prediction_loss_fake_1\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  discriminator = discriminator_cwgan()  # without initialization\n","\n","\n","  def generator_label(embedding, decoder):\n","      label = Input((1,), dtype='int32')\n","      latent = Input((latent_dim,))\n","      labeled_latent = embedding([latent, label])\n","      gen_img = decoder(labeled_latent)\n","      model = Model([latent, label], gen_img)\n","\n","      return model\n","\n","  generator= generator_label(em, deca)  # initialized with Decoder and Embedding\n","  cgf=classifier_gan_first()\n","  cgs=classifier_gan_second()\n","\n","  classifier = classifier_gan(cgf, cgs)\n","  generator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  discriminator_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  classifier_optimizer = Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.9)\n","  batch_size=batch_number\n","  latent_dim=128\n","\n","\n","\n","  def plt_img(generator):\n","      np.random.seed(42)\n","      latent_gen = np.random.normal(size=(n_classes, latent_dim))\n","\n","      x_real = x_test * 0.5 + 0.5\n","      n = n_classes\n","\n","      plt.figure(figsize=(2*n, 2*(n+1)))\n","      for i in range(n):\n","          # display original\n","          ax = plt.subplot(n+1, n, i + 1)\n","          if channel == 3:\n","              plt.imshow(x_real[y_test==i][1].reshape(64, 64, channel))\n","          else:\n","              plt.imshow(x_real[y_test == i][1].reshape(64, 64))\n","              plt.gray()\n","          ax.get_xaxis().set_visible(False)\n","          ax.get_yaxis().set_visible(False)\n","          for c in range(n):\n","              #decoded_imgs = generator.predict([latent_gen, np.ones(n)*c])\n","              decoded_imgs = generator([latent_gen, np.ones(n)*c],training=True)\n","              decoded_imgs=np.asarray(decoded_imgs)\n","              decoded_imgs = decoded_imgs * 0.5 + 0.5\n","              # display generation\n","              ax = plt.subplot(n+1, n, (i+1)*n + 1 + c)\n","              if channel == 3:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64, channel))\n","              else:\n","                  plt.imshow(decoded_imgs[i].reshape(64, 64))\n","                  plt.gray()\n","              ax.get_xaxis().set_visible(False)\n","              ax.get_yaxis().set_visible(False)\n","      #plt.savefig('bagan_gp_results/generated_plot_%d.png' % epoch)\n","      plt.show()\n","      return\n","\n","\n","\n","  generator.compile(optimizer=generator_optimizer)\n","  discriminator.compile(optimizer=discriminator_optimizer)\n","  classifier.compile(optimizer=classifier_optimizer)\n","\n","\n","  train_loss=[]\n","  test_loss=[]\n","  classifier_loss_function = []\n","  recall_loss = []\n","  precision_loss = []\n","\n","\n","  x_train = x_train.astype('float32')\n","  ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","  ds = ds.repeat().batch(batch_number)\n","  train_iter = iter(ds)\n","\n","\n","\n","  for epoch in range(150):\n","      print(\"epoch\")\n","      print(epoch)\n","      d_total=0\n","      g_total=0\n","      c_total=0\n","      c_total1=0\n","      for batch in range(iter_number):\n","          real_images, labels = next(train_iter)\n","          labels1=labels.numpy()\n","          labels=tf.cast(labels,dtype=tf.int32)\n","          for i in range(10):\n","              random_latent_vectors = tf.random.normal(shape= (batch_size, latent_dim))\n","              fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","              wrong_labels=np.zeros(labels1.shape[0])\n","              for i in range(labels1.shape[0]):\n","                wrong=[0,1]\n","                wrong.remove(labels1[i])\n","                wrong_labels[i]=(np.array(random.sample(wrong,1)))\n","\n","              tf.convert_to_tensor(wrong_labels)\n","              tf.cast(wrong_labels,dtype='int32')\n","\n","              with tf.GradientTape() as tape:\n","                  fake_images = generator([random_latent_vectors, fake_labels], training=True)\n","                  fake_logits = discriminator([fake_images, fake_labels], training=True)\n","                  real_logits = discriminator([real_images, labels], training=True)\n","\n","                  wrong_label_logits = discriminator([real_images, wrong_labels], training=True)\n","\n","                  d_cost = discriminator_loss(real_logits,fake_logits,wrong_label_logits)\n","                  alpha = tf.random.uniform([batch_size, 1, 1, 1], 0.0, 1.0)\n","                  diff = fake_images - real_images\n","                  interpolated = real_images + alpha * diff\n","\n","                  with tf.GradientTape() as gp_tape:\n","                      gp_tape.watch(interpolated)\n","                      pred = discriminator([interpolated, labels], training=True)\n","\n","                  grads = gp_tape.gradient(pred, [interpolated])[0]\n","                  norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","                  gp = tf.reduce_mean((norm - 1.0) ** 2)\n","\n","                  d_loss = d_cost+gp*10\n","                  d_total=d_total+d_loss\n","\n","              d_gradient = tape.gradient(d_loss, discriminator.trainable_variables)\n","              discriminator_optimizer.apply_gradients(zip(d_gradient, discriminator.trainable_variables))\n","\n","          ########################### Train the Generator ###########################\n","          # Get the latent vector\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n","          fake_labels = tf.random.uniform((batch_size,), 0, n_classes,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape:\n","              generated_images = generator([random_latent_vectors, fake_labels], training=True)\n","              gen_img_logits = discriminator([generated_images, fake_labels], training=True)\n","              gen_prediction = classifier([generated_images], training=True)\n","\n","              fake_loss,prediction_loss_gen,boundary,g_loss = generator_loss(gen_img_logits,fake_labels,gen_prediction,epoch)\n","              g_total=g_total+g_loss\n","          gen_gradient = tape.gradient(g_loss, generator.trainable_variables)\n","          generator_optimizer.apply_gradients(zip(gen_gradient, generator.trainable_variables))\n","\n","\n","          l0=labels[labels==0].shape[0]\n","          l1=labels[labels==1].shape[0]\n","\n","          classify_train_l0 = l0-l0\n","          classify_train_l1 = l0-l1\n","\n","      #     ########################### Train the Classifier ###########################\n","          random_latent_vectors_0 = tf.random.normal(shape=(classify_train_l0, latent_dim))\n","          random_latent_vectors_1 = tf.random.normal(shape=(classify_train_l1, latent_dim))\n","          classify_0 = tf.random.uniform((classify_train_l0,), 0, 1,dtype='int32')\n","          classify_1 = tf.random.uniform((classify_train_l1,), 1, 2,dtype='int32')\n","\n","\n","          with tf.GradientTape() as tape_classifier:\n","\n","            fake_prediction_000 = generator([random_latent_vectors_0, classify_0], training=True)\n","            fake_prediction_111 = generator([random_latent_vectors_1, classify_1], training=True)\n","\n","\n","            real_prediction = classifier([real_images], training=True)\n","            fake_prediction_0 = classifier([fake_prediction_000], training=True)\n","            fake_prediction_1 = classifier([fake_prediction_111], training=True)\n","\n","            c_loss = classifier_loss(labels,real_prediction,classify_0, fake_prediction_0,classify_1, fake_prediction_1)\n","            c_total=c_total+c_loss\n","          classification_gradient = tape_classifier.gradient(c_loss,classifier.trainable_variables)\n","          classifier_optimizer.apply_gradients(zip(classification_gradient, classifier.trainable_variables))\n","\n","      # print(d_loss)\n","      # print(g_loss)\n","      # print(c_loss)\n","\n","\n","      #plt_img(generator)\n","      train_prediction = classifier.predict([x_train])\n","      train_prediction_label=(tf.math.argmax(train_prediction,axis=1))\n","      print(\"F_Score_Train\")\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_train, train_prediction_label,labels=[0,1,2]))\n","      train_loss =  np.append(train_loss, f1_score(y_train, train_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      test_prediction = classifier.predict([x_test])\n","      test_prediction_label=(tf.math.argmax(test_prediction,axis=1))\n","      print(\"F_Score_Test\")\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      # print(confusion_matrix(y_test, test_prediction_label,labels=[0,1,2]))\n","      test_loss =  np.append(test_loss, f1_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"Recall_Test\")\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      recall_loss =  np.append(recall_loss, recall_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      print(\"precision_Test\")\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average=None))\n","      print(precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","      precision_loss =  np.append(precision_loss, precision_score(y_test, test_prediction_label.numpy(),labels=[0,1], average='macro'))\n","\n","      # print(\"C_loss\")\n","      # print(c_total/iter_number)\n","      classifier_loss_function = np.append(classifier_loss_function,c_total/iter_number)\n","\n","      if epoch%5 ==0:\n","        plt_img(generator)\n","        plt.figure(figsize=(8, 8))\n","        color = plt.get_cmap('tab10')\n","        A=tf.concat([fake_prediction_000,fake_prediction_111,real_images],0)\n","        B=tf.concat([classify_0,classify_1,labels],0)\n","        latent = cgf.predict(A)  # with Encoder\n","        tsne_model = TSNE(n_components=2, init='random', random_state=0)\n","        new_values = tsne_model.fit_transform(latent)\n","        x = []\n","        y = []\n","        for value in new_values:\n","          x.append(value[0])\n","          y.append(value[1])\n","        x = np.array(x)\n","        y = np.array(y)\n","\n","        x_generated = x[:-1 * batch_size]\n","        y_generated = y[:-1 * batch_size]\n","        x_real = x[-1 * batch_size:]\n","        y_real = y[-1 * batch_size:]\n","        generated_label = B[:-1 * batch_size]\n","        real_label = B[-1 * batch_size:]\n","\n","\n","        loop = 0\n","        markers = ['o', 'x']\n","        for x, y, l in [(x_real, y_real, real_label), (x_generated, y_generated, generated_label)]:\n","          marker = markers[loop]\n","          loop += 1\n","          print(l)\n","          # print(batch_size)\n","          # print(x_real.shape)\n","          # print(real_label.shape)\n","          for c in range(n_classes):\n","              l3=np.asarray(l)\n","              plt.scatter(x[l3 == c], y[l3 == c], marker=marker, s=50, c=np.array([color(c)]))\n","\n","        plt.show()\n","\n","\n","  plt.plot(train_loss)\n","  plt.plot(test_loss)\n","  plt.legend([\"train\",\"test\"])\n","  plt.ylim(0,1)\n","  plt.show()\n","  plt.plot(classifier_loss_function)\n","  plt.legend([\"classfier\"])\n","  plt.show()\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/TRAIN/PROPOSE')\n","  np.savetxt(\"train_%d_EBM.CSV\" % number, np.asarray(train_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/FSCORE/PROPOSE')\n","  np.savetxt(\"test_%d_EBM.CSV\" % number, np.asarray(test_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/CLASSIFICATION/PROPOSE')\n","  np.savetxt(\"classifier_%d_EBM.CSV\" % number, np.asarray(classifier_loss_function), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/RECALL/PROPOSE')\n","  np.savetxt(\"recall_%d_EBM.CSV\" % number, np.asarray(recall_loss), delimiter=\",\")\n","  os.chdir('/content/drive/My Drive/Colab Notebooks/Johnson/glyco_scale/glyco/imbalanceGAN/RESULT_0911/RESULT/PRECISION/PROPOSE')\n","  np.savetxt(\"precision_%d_EBM.CSV\" % number, np.asarray(precision_loss), delimiter=\",\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZAenH-bm9kCa"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"JmdPP-Fy9kEq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AYvpF9_t9kIl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNj+gB9DA1wMO24LnYG4Ong"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}